from typing import Tuple
import pandas as pd
from sklearn.datasets import make_friedman1, make_classification
import numpy as np
from ucimlrepo import fetch_ucirepo
import numpy as np
from scipy import sparse
from sklearn.utils import check_random_state
from typing import Tuple, Union


def generate_large_synthetic_dataset(
    n_samples: int = 100_000,
    n_features: int = 20_000,
    informative_fraction: float = 0.2,
    class_prior: float = 0.5,
    mean_shift: float = 2.0,
    noise_std: float = 1.0,
    density: float = 0.05,
    return_sparse: bool = True,
    random_state: Union[int, np.random.RandomState, None] = None,
) -> Tuple[Union[np.ndarray, sparse.csr_matrix], np.ndarray]:
    """
    Create a *very* high-dimensional binary-classification dataset suitable for the
    ‚Äúlarge-d check-C‚Äù experiments (point 4).

    Data model
    ----------
    ‚Ä¢ y ‚àº Bernoulli(class_prior)
    ‚Ä¢ X | y = 0  ‚àº ùí©(0, I)
    ‚Ä¢ X | y = 1  ‚àº ùí©(Œº, I)‚ÄÉwith Œº_j = mean_shift for j in informative set
    ‚Ä¢ After drawing dense Gaussian samples, we randomly zero-out entries so that
      the final feature matrix has the requested *density* (‚âà fraction non-zero).

    Parameters
    ----------
    n_samples          total observations (rows)
    n_features         dimensionality d
    informative_fraction
                       fraction of features that carry a mean shift
    class_prior        P(y = 1)
    mean_shift         magnitude of the signal in informative coords
    noise_std          œÉ of the Gaussian noise
    density            final non-zero ratio (0 < density ‚â§ 1)
    return_sparse      if True ‚Üí CSR matrix; else dense ndarray
    random_state       int or `np.random.RandomState`

    Returns
    -------
    X   shape (n_samples, n_features); CSR or ndarray
    y   shape (n_samples,), values in {-1, +1}
    """
    rng = check_random_state(random_state)

    y = rng.binomial(1, class_prior, n_samples)
    y = 2 * y - 1

    n_informative = int(np.round(informative_fraction * n_features))
    informative_idx = rng.choice(n_features, n_informative, replace=False)

    X = rng.normal(loc=0.0, scale=noise_std, size=(n_samples, n_features))

    pos_mask = y == 1
    X[np.ix_(pos_mask, informative_idx)] += mean_shift

    if density < 1.0:
        # keep each entry with probability = density
        keep = rng.uniform(size=X.shape) < density
        X = np.where(keep, X, 0.0)

    if return_sparse:
        X = sparse.csr_matrix(X, dtype=np.float32)

    return X, y


def generate_friedman1_dataset(
    n_samples: int = 200,
    n_features: int = 10,
    n_noise_features: int = 190,
    noise: float = 1.0,
    random_state: int = 0,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate a synthetic regression dataset based on the Friedman #1 function.

    Args:
        n_samples (int): Number of data points to generate.
        n_features (int): Total number of features passed (only the first 5 of these are informative).
        n_noise_features (int): Additional pure-noise features to append.
        noise (float): Standard deviation of the Gaussian noise Œµ.
        random_state (int): Seed for reproducibility.

    Returns:
        X (np.ndarray of shape (n_samples, n_features + n_noise_features)):
            Feature matrix consisting of `n_features` Friedman inputs
            plus `n_noise_features` uniform noise columns.
        y (np.ndarray of shape (n_samples,)):
            Target vector generated by the Friedman #1 formula.
    """
    # Base dataset: 10 features, first 5 informative, default œÉ=1 noise
    X, y = make_friedman1(
        n_samples=n_samples,
        n_features=n_features,
        noise=noise,
        random_state=random_state,
    )

    # To stress-test at higher dims, append n noise features:
    X = np.hstack([X, np.random.rand(X.shape[0], n_noise_features)])

    return X, y


def load_ucirepo_dataset(dataset_id: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    Load a dataset from the UCI repository using ucirepo.fetch_ucirepo,
    returning the feature array X and target array y.

    Args:
        dataset_id (str): Name or ID of the UCI dataset.

    Returns:
        X (np.ndarray): Feature matrix of shape (n_samples, n_features).
        y (np.ndarray): Target vector of shape (n_samples,).
    """
    # Fetch the dataset as arrays
    ds = fetch_ucirepo(id=dataset_id)
    X = ds.data.features.to_numpy()
    y = ds.data.targets.to_numpy().ravel()

    # Standarize X to zero mean and unit variance
    means = X.mean(axis=0)
    stds = X.std(axis=0, ddof=0)
    #    avoid divide-by-zero
    stds[stds == 0] = 1.0
    #    center & scale
    X = (X - means) / stds

    # Ensure it's a binary classification problem
    classes = np.unique(y)
    if classes.size != 2:
        raise ValueError(
            f"2 classes in target expected, but found {classes.size}: {classes}"
        )

    # Map the first class ‚Üí -1, the second ‚Üí +1
    y = np.where(y == classes[0], -1, 1)

    return X, y


def syntetic_dataset_generation(
    p: float, n: int, d: int, g: float
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate a synthetic dataset as specified.

    The function generates a binary target variable y from a Bernoulli distribution with class prior probability p.
    The feature matrix X is generated such that:
      - For y = 0, X follows a d-dimensional multivariate normal with mean vector of zeros and covariance matrix S,
        where S[i, j] = g^|i-j|.
      - For y = 1, X follows a d-dimensional multivariate normal with mean vector (1, 1/2, 1/3, ..., 1/d) and the
        same covariance structure as for y = 0.

    Args:
        p (float): Class prior probability.
        n (int): Total number of samples to generate.
        d (int): Dimensionality of the feature vectors.
        g (float): Covariance matrix parameter (defines correlation structure).

    Returns:
        Tuple[np.ndarray, np.ndarray]:
            - X: An (n x d) array of feature vectors.
            - y: A 1D array of length n containing binary class labels.
    """
    # Generate binary class variable from a Bernoulli distribution.
    y = np.random.binomial(1, p, n)

    # Mean vectors for the two classes.
    mean_0 = np.zeros(d)
    mean_1 = np.array([1 / (i + 1) for i in range(d)])

    # Build covariance matrix S with S[i, j] = g^|i - j|
    cov = np.zeros((d, d))
    for i in range(d):
        for j in range(d):
            cov[i, j] = g ** abs(i - j)

    # Initialize feature matrix X.
    X = np.zeros((n, d))
    mask_0 = y == 0
    mask_1 = y == 1

    # Generate feature vectors for each class.
    X[mask_0] = np.random.multivariate_normal(mean_0, cov, mask_0.sum())
    X[mask_1] = np.random.multivariate_normal(mean_1, cov, mask_1.sum())

    return X, y


def generate_high_dim_classification(
    n_samples: int = 500,
    n_features: int = 1000,
    n_informative: int = 600,
    n_redundant: int = 0,
    n_repeated: int = 0,
    n_classes: int = 2,
    class_sep: float = 1.0,
    flip_y: float = 0.01,
    random_state: int = 0,
):
    """
    Generate a high-dimensional binary classification problem.

    Returns:
        X_train, X_test, y_train, y_test
    """
    X, y = make_classification(
        n_samples=n_samples,
        n_features=n_features,
        n_informative=n_informative,
        n_redundant=n_redundant,
        n_repeated=n_repeated,
        n_classes=n_classes,
        class_sep=class_sep,
        flip_y=flip_y,
        random_state=random_state,
    )
    return X, y
